{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etUt8t6gv8HK"
   },
   "source": [
    "# Self Stabilising Priors for Robust Bayesian Deep Learning\n",
    "\n",
    "This notebook is designed to demonstrate the basic implementation of a normal Bayesian Neural Network and a Bayesian Neural Network with Stabilizing Priors for the CIFAR10 Data Set\n",
    "\n",
    "\n",
    "This notebook is based on  https://github.com/senya-ashukha/sparse-vd-pytorch/blob/master/svdo-solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTbCA4-eNt2v"
   },
   "source": [
    "<img src=\"https://github.com/felixmcgregor/Bayesian-Neural-Networks-with-self-stabilising-priors/blob/master/intuition1.png?raw=1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVnbfF7pwbeH"
   },
   "source": [
    "### Installation and to run on google colab  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2hTKqOQwTkB"
   },
   "outputs": [],
   "source": [
    "# Logger\n",
    "#!pip install tabulate -q\n",
    "#from google.colab import files\n",
    "#src = list(files.upload().values())[0]\n",
    "#open('logger.py','wb').write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZepqeOrNt29"
   },
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iiCiVLaJv8HV"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICMEDWnov8HW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BswA42agNt3O"
   },
   "outputs": [],
   "source": [
    "# The CPU/GPU modified code done by our group\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKDexGZpNt3T"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "width = 256*3\n",
    "input_shape = 3*32*32\n",
    "#input_shape = 28*28\n",
    "output_size = 10\n",
    "batch_size = 100\n",
    "init_var = 0.001\n",
    "\n",
    "n_samples = 20\n",
    "kl_weight = 1.0\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2NfHM-9Nt3Z"
   },
   "source": [
    "## Our proposed self stabilising layer for Bayesian Neural Network\n",
    "For the stabilising prior to be effective we sample from a reparametrised, $\\tilde{q}(W)$, which is the product of the current posterior, $q(W)$, and the prior, $p(W)$. This allows the the influence of the prior on the forward pass so we can propagate cleaner signals. \n",
    "\n",
    "The other main differences between this layer and a normal Bayesian layer is the update prior function, because our prior adapts based on the current settings of the weights to stabilise the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdIXmm0INt3a"
   },
   "outputs": [],
   "source": [
    "class SelfStabilisingLayer(nn.Module):\n",
    "    '''\n",
    "    Iteratively updating self stabilising prior.\n",
    "    Fully factorised Gaussian priors and posteriors.\n",
    "    Local reparametrisation trick.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features, out_features, init_var=0.001, prior_var=0.02):\n",
    "        super(SelfStabilisingLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.W = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.log_sigma = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = Parameter(torch.Tensor(1, out_features))\n",
    "\n",
    "        # initialisation values of parameters\n",
    "        self.init_var = np.log(init_var)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.log_sigma.data.fill_(self.init_var / 2)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "        # He initialisation\n",
    "        init = np.sqrt(2 / self.in_features)\n",
    "        self.W.data.normal_(0, init)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # local reparametrisation trick with new parameters from q_tilde(W)\n",
    "        # i.e. use self.new_mu and self.new_sigma_sq\n",
    "        lrt_mean = F.linear(x, self.new_mu) + self.bias\n",
    "        lrt_std = torch.sqrt(F.linear(x * x, self.new_sigma_sq) + 1e-8)\n",
    "        eps = lrt_std.data.new(lrt_std.size()).normal_()\n",
    "        pre_activation = lrt_mean + lrt_std * eps\n",
    "\n",
    "        return pre_activation\n",
    "    \n",
    "\n",
    "    def update_prior(self):\n",
    "        \n",
    "        #####################################################################\n",
    "        # Main difference between normal BNN and Stabilising prior\n",
    "        #####################################################################\n",
    "\n",
    "        # Sum of all incoming nodes to specific hidden units\n",
    "        mu_L = torch.sum(self.W, dim=1)\n",
    "        sig_sq_L = torch.sum(torch.exp(self.log_sigma * 2.0), dim=1)\n",
    "\n",
    "        # PRIOR VARIANCE        \n",
    "        gamma = 2- (1-1/math.pi) *mu_L * mu_L\n",
    "        self.prior_var = (gamma * sig_sq_L)/(sig_sq_L - gamma)\n",
    "        self.prior_var = self.prior_var / self.in_features\n",
    "\n",
    "        # shared prior across all weights feeding into the same hidden unit\n",
    "        self.prior_var = self.prior_var.expand(self.in_features, self.prior_var.shape[0]).t()\n",
    "        self.prior_var = torch.abs(self.prior_var)\n",
    "        \n",
    "        \n",
    "        # PRIOR MEAN (mean preserving)\n",
    "        self.prior_mean = self.W\n",
    "\n",
    "        \n",
    "        # PRODUCT, set the parameters from which we will be sampling q_tilde(W)\n",
    "        self.new_mu, self.new_sigma_sq = multipy_gaussian(self.W, self.prior_mean, \n",
    "                                                            torch.exp(self.log_sigma * 2.0), self.prior_var)\n",
    "\n",
    "        \n",
    "    def kl_reg(self):\n",
    "\n",
    "        # cross entropy term\n",
    "        sigma_sq = torch.exp(self.log_sigma.view(-1) * 2)\n",
    "        new_sigma_sq = torch.exp(self.new_sigma_sq.view(-1))\n",
    "        pi = math.pi\n",
    "\n",
    "        H = 0.5 * torch.log(2 * pi * sigma_sq) + (new_sigma_sq / sigma_sq)\n",
    "        H = torch.sum(H)\n",
    "\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSLHrORyNt3f"
   },
   "source": [
    "## Normal Bayesian Neural Network layer\n",
    "Non-conjugate Gaussian prior and Gaussian posterior. We also make use of the Local Reparametrisation trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKlzUBfiNt3g"
   },
   "outputs": [],
   "source": [
    "class LocalReparametrisationLayer(nn.Module):\n",
    "    '''\n",
    "    Doubly stochastic Variational Bayes for non-conjugate inference.\n",
    "    Fully factorised Gaussian priors and posteriors.\n",
    "    Local reparametrisation trick.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, init_var=0.001, prior_var=0.0001):\n",
    "        super(LocalReparametrisationLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.W = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.log_sigma = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = Parameter(torch.Tensor(1, out_features))\n",
    "\n",
    "        # add priors \n",
    "        self.prior_mean = torch.Tensor([0]).to(device)\n",
    "        self.prior_var = torch.Tensor([prior_var]).to(device)\n",
    "\n",
    "        # initialisation values of parameters\n",
    "        self.init_var = np.log(init_var)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.log_sigma.data.fill_(self.init_var / 2)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "        # critical initialisation for normal Bayesian Neural networks\n",
    "        init = np.sqrt(np.abs((2 - self.in_features * np.exp(self.init_var)) / self.in_features))\n",
    "        self.W.data.normal_(0, init)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # local reparametrisation trick \n",
    "        lrt_mean = F.linear(x, self.W) #+ self.bias\n",
    "        lrt_std = torch.sqrt(F.linear(x * x, torch.exp(self.log_sigma * 2.0)) + 1e-8)\n",
    "        eps = lrt_std.data.new(lrt_std.size()).normal_()\n",
    "        pre_activation = lrt_mean + lrt_std * eps\n",
    "\n",
    "        if self.training:\n",
    "            self.signal_variance = pre_activation.var(dim=1)[0].data.cpu().numpy()          \n",
    "\n",
    "        return pre_activation\n",
    "    \n",
    "    def kl_reg(self):\n",
    "\n",
    "        # KL divergence \n",
    "        mean = self.W.view(-1)\n",
    "        sigma = torch.exp(self.log_sigma).view(-1)\n",
    "\n",
    "        prior_sigma = torch.sqrt(self.prior_var).view(-1)\n",
    "        prior_mean = self.prior_mean.view(-1)\n",
    "\n",
    "        p = torch.distributions.normal.Normal(prior_mean, prior_sigma)\n",
    "        q = torch.distributions.normal.Normal(mean, sigma)\n",
    "\n",
    "        kl = torch.distributions.kl.kl_divergence(q, p)\n",
    "\n",
    "        kl = torch.sum(kl)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmSSuE8jNt3l"
   },
   "source": [
    "### Generic ReLU network architecture where we specify the type of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mGfHQ4Nv8Ha"
   },
   "outputs": [],
   "source": [
    "# Define a simple fully connected ReLU Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_type, input_size, width=256, init_var=0.001):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc_in = layer_type(input_size, width, init_var=init_var)\n",
    "        self.fc_h1 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_h2 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_h3 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_h4 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_h5 = layer_type(width, width, init_var=init_var)\n",
    "        self.fc_out = layer_type(width,  output_size, init_var=init_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc_in(x))\n",
    "        x = F.relu(self.fc_h1(x))\n",
    "        x = F.relu(self.fc_h2(x))\n",
    "        x = F.relu(self.fc_h3(x))\n",
    "        x = F.relu(self.fc_h4(x))\n",
    "        x = F.relu(self.fc_h5(x))\n",
    "        x = F.log_softmax(self.fc_out(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def update_priors(self):\n",
    "        if hasattr(self.fc_h1, 'update_prior'):\n",
    "            self.fc_in.update_prior()\n",
    "            self.fc_out.update_prior()\n",
    "            self.fc_h1.update_prior()\n",
    "            self.fc_h2.update_prior()\n",
    "            self.fc_h3.update_prior()\n",
    "            self.fc_h4.update_prior()\n",
    "            self.fc_h5.update_prior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k7SlUjpNt3r"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6C_w0KxNt3t"
   },
   "outputs": [],
   "source": [
    "# Define New Loss Function -- SGVLB \n",
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size, batch_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = batch_size / train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, output, target, kl_weight=1.0):\n",
    "        assert not target.requires_grad\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_reg'):\n",
    "                kl = kl + module.kl_reg()\n",
    "        kl = kl * self.num_batches\n",
    "        kl = kl / (self.batch_size*self.train_size)\n",
    "        #kl = kl_weight * kl\n",
    "        return F.nll_loss(output, target, reduction='mean') + kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVFfET-INt3y"
   },
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fi-O-SFv8Hc"
   },
   "outputs": [],
   "source": [
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N79pXExRs_o"
   },
   "outputs": [],
   "source": [
    "# Defining Dataloaders for CIFAR10 data set\n",
    "def get_cifar(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBhtWVAjNt36"
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTkfKuJ1Nt37"
   },
   "outputs": [],
   "source": [
    "def multipy_gaussian(mean1, mean2, var1, var2):\n",
    "\n",
    "    # calculate variance\n",
    "    new_var = 1 / ((1 / var1) + (1 / var2))\n",
    "\n",
    "    # calculate mean\n",
    "    new_mu = new_var * (mean2 / var2 + mean1 / var1)\n",
    "\n",
    "    return new_mu, new_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NjdhaeVNt4A"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3ov5OUxNt4B"
   },
   "outputs": [],
   "source": [
    "# training loop with logging\n",
    "def train(model, epochs, optimizer, train_loader, test_loader, loss_fn, logger):\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.train() \n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            #####################################################################\n",
    "            # Training\n",
    "            #####################################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, input_shape)\n",
    "              \n",
    "            # update priors\n",
    "            model.update_priors()\n",
    "\n",
    "            # forward prop\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "\n",
    "            # backprop\n",
    "            loss = loss_fn(output, target, kl_weight)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # training loss and accuracy (training accuracy does not reflect ensemble)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += torch.sum(pred.eq(target))\n",
    "\n",
    "        #####################################################################\n",
    "        # Evaluate on test set\n",
    "        #####################################################################\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, avg_test_acc, total_brier = 0, 0, 0\n",
    "        total_90, total_70, total_50 = 0, 0, 0\n",
    "        for tbatch_idx, (data, target) in enumerate(test_loader):\n",
    "            # prep\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, input_shape)\n",
    "\n",
    "            # average over weights with samples of different parameters\n",
    "            probs, test_loss = 0, 0\n",
    "            for i in range(n_samples):\n",
    "                output = model(data)\n",
    "                probs += output.data\n",
    "                test_loss += float(loss_fn(output, target).item())\n",
    "            mean_probs = probs / n_samples\n",
    "            avg_test_loss = test_loss / n_samples\n",
    "            pred = torch.argmax(mean_probs, dim=1)\n",
    "            avg_test_acc += torch.sum(pred == target)\n",
    "\n",
    "        \n",
    "\n",
    "        #####################################################################\n",
    "        # Logging\n",
    "        #####################################################################\n",
    "        # log training and test loss and accuracy\n",
    "        logger.add_scalar(epoch, 'trlos', train_loss)\n",
    "        logger.add_scalar(epoch, 'telos', avg_test_loss)\n",
    "\n",
    "        logger.add_scalar(epoch, 'tracc', (float(train_acc) / (batch_size * (batch_idx + 1)) * 100))\n",
    "        logger.add_scalar(epoch, 'teacc', float(avg_test_acc) / len(test_loader.dataset) * 100)\n",
    "\n",
    "        logger.iter_info()\n",
    "    logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kkz6v5fNt4F"
   },
   "source": [
    "## Experiment on CIFAR10 Data Set\n",
    "A very simple example on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umbKtLgXNt4G"
   },
   "outputs": [],
   "source": [
    "width = 1024\n",
    "init_var = 0.01\n",
    "epochs = 10\n",
    "#input_shape = 28*28 #MNIST\n",
    "input_shape = 3*32*32 #CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKb74m5bNt4L"
   },
   "source": [
    "#### 1. Normal BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "S7HkpvRVv8Hh",
    "outputId": "a3b5f889-58f5-4c4c-d579-3ead3a3db5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "  epoch    trlos    telos    tracc    teacc\n",
      "-------  -------  -------  -------  -------\n",
      "      1  37710.6      6.1     10.0      9.7\n",
      "      2   2097.8      2.9     10.0      9.7\n",
      "      3   1746.1      3.1     10.0     10.2\n",
      "      4   1586.8      2.6     10.1      9.8\n",
      "      5   1504.1      2.7     10.2     10.3\n",
      "      6   1437.9      2.6     10.1      9.9\n",
      "      7   1396.0      2.6     10.1      9.7\n",
      "      8   1380.4      2.6      9.8     10.0\n",
      "      9   1361.7      2.6      9.8      9.7\n",
      "     10   1356.1      2.6     10.1      9.8\n",
      "The log/output/model have been saved to: ./logs/python3.6-dist-packages-ipykernel_launcher.py-LRTNet-05-29-01-03-lqlxs + .csv/.out/.cpt\n"
     ]
    }
   ],
   "source": [
    "model = Net(LocalReparametrisationLayer, input_shape, width=width, init_var=init_var)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "logger = Logger(name='LRTNet')\n",
    "\n",
    "train_loader, test_loader = get_cifar(batch_size=batch_size)\n",
    "loss_fn = SGVLB(model, len(train_loader.dataset), batch_size)\n",
    "\n",
    "if device == 'cuda':\n",
    "    model.cuda()\n",
    "    \n",
    "train(model, epochs, optimizer, train_loader, test_loader, loss_fn, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewRtHSL2Nt4T"
   },
   "source": [
    "#### 2. Self stabilising prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "v6mf7WjhJIqA",
    "outputId": "49380eed-2459-40bf-d7bc-6f49cfa6f7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "  epoch    trlos    telos    tracc    teacc\n",
      "-------  -------  -------  -------  -------\n",
      "      1   1387.9      2.5     13.1     17.7\n",
      "      2   1220.0      2.5     16.8     18.4\n",
      "      3   1194.5      2.4     17.5     18.5\n",
      "      4   1193.0      2.3     16.6     17.9\n",
      "      5   1158.7      2.3     17.4     19.0\n",
      "      6   1138.2      2.3     18.8     20.3\n",
      "      7   1119.1      2.2     20.2     22.4\n",
      "      8   1088.4      2.2     22.9     26.3\n",
      "      9   1064.6      2.1     24.2     25.7\n",
      "     10   1043.2      2.1     25.7     30.3\n",
      "The log/output/model have been saved to: ./logs/python3.6-dist-packages-ipykernel_launcher.py-StabilisedNet-05-29-01-07-xlriw + .csv/.out/.cpt\n"
     ]
    }
   ],
   "source": [
    "kl_weight = 1.0\n",
    "\n",
    "model = Net(SelfStabilisingLayer, input_shape, width=width, init_var=init_var)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "logger = Logger(name='StabilisedNet')\n",
    "\n",
    "train_loader, test_loader = get_cifar(batch_size=batch_size)\n",
    "loss_fn = SGVLB(model, len(train_loader.dataset), batch_size)\n",
    "\n",
    "if device == 'cuda':\n",
    "    model.cuda()\n",
    "    \n",
    "train(model, epochs, optimizer, train_loader, test_loader, loss_fn, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MB-_z3Adda3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3. Sparse Variational Dropout.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
